{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8p6PBfK3j-b"
      },
      "source": [
        "#Loading and preparing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ryWsO9TY3wqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee5ba5e-a8eb-4b62-8088-3d85a077b121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qqq openai cohere tiktoken langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PakIJUCq3wP4"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('openai_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wq_jaPnA3wbg"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.schema import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LdyLJQwq3v2V"
      },
      "outputs": [],
      "source": [
        "# Langchain for chat models: without chains\n",
        "def generate_response_langchain(prompt:str, system:str = \"\", temperature:float = 0.0, max_tokens:int = 50):\n",
        "\n",
        "  chat = ChatOpenAI(model_name='gpt-3.5-turbo-1106', temperature=temperature, max_tokens=max_tokens, openai_api_key=openai_api_key)\n",
        "  messages = [SystemMessage(content=system),\n",
        "              HumanMessage(content=prompt),\n",
        "              ]\n",
        "  return chat(messages).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749734fe-c78c-49a8-dee4-8f9c547bf729",
        "id": "_105udtYzYDJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Hello there!\"\n",
        "response = generate_response_langchain(prompt)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA_wu9uG3csV"
      },
      "source": [
        "# Zero-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Langchain for chat models: using chains!\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"What is a good name for a company that makes {product}?\"\n",
        ")\n",
        "chain = prompt | ChatOpenAI(temperature=0.7, openai_api_key=openai_api_key)\n",
        "# Feel free the change test out your own product of choice, you can swap out the colorful socks in the following line with you choice of product\n",
        "print(chain.invoke({\"product\": \"colorful socks\"}).content)"
      ],
      "metadata": {
        "id": "mNulZvZjyrN2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607c5f86-b730-4e56-ec23-12b4a54a5f95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rainbow Threads\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero-shot is useful, but sometimes it might not work...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kg0bxZ86be2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate.from_template(\n",
        "    \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 2. {question}\"\n",
        ")\n",
        "chain = prompt | ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "print(chain.invoke({\"question\": \"True or false?\"}).content)"
      ],
      "metadata": {
        "id": "xHg2NLkkbdxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59aef70a-6d7a-4f8d-d68a-f72bca2c6915"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW-xE2AG4Rua"
      },
      "source": [
        "#Few-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\", \"output\": \"The answer is False.\"},\n",
        "    {\"input\": \"The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\", \"output\": \"The answer is True.\"},\n",
        "    {\"input\": \"The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\", \"output\": \"The answer is True.\"},\n",
        "    {\"input\": \"The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\", \"output\": \"The answer is False.\"},\n",
        "    ]\n",
        "\n",
        "# This is a prompt template used to format each individual example.\n",
        "example_prompt = ChatPromptTemplate.from_messages([(\"human\", \"{input}\"), (\"ai\", \"{output}\"),])\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "question = \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 2. True or false\"\n",
        "\n",
        "final_prompt = ChatPromptTemplate.from_messages([\n",
        "    # (\"system\", \"You are a wondrous wizard of math.\"), #Optional\n",
        "     few_shot_prompt, (\"human\", \"{question}\"),\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "ZuJRnclgcHGm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = final_prompt | ChatOpenAI(temperature=0.0, openai_api_key=openai_api_key)\n",
        "print(chain.invoke({\"question\": question}).content)"
      ],
      "metadata": {
        "id": "f0Rv9fZRcWJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b68ddb4-a3d6-4654-9856-575b0d0266ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer is True.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chain-of-thought Prompting"
      ],
      "metadata": {
        "id": "g4QMQ7o06SAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zero-shot chain-of-thought"
      ],
      "metadata": {
        "id": "VPqsy03MfYpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Note that the prompt is almost identical to the zero-shot example. The only different is to add \"Let's think step by step\" at the end of the prompt.\n",
        "\n"
      ],
      "metadata": {
        "id": "3cfw-1Fffz0o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mev3jE5yQroM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2cd511a-e68a-4216-858c-0f8fdd687dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's add up the odd numbers in the group: 15 + 5 + 13 + 7 = 40.\n",
            "\n",
            "Since 40 is an even number, the statement \"The odd numbers in this group add up to an even number\" is true.\n"
          ]
        }
      ],
      "source": [
        "prompt = PromptTemplate.from_template(\"{question}\")\n",
        "\n",
        "question = \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 2. True or false? Let's think step by step\"\n",
        "chain = prompt | ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "print(chain.invoke({\"question\": question}).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Few-shot chain-of-thought"
      ],
      "metadata": {
        "id": "w0w8s1N6fd5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\"input\": \"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\", \"output\": \"Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\"},\n",
        "    {\"input\": \"The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\", \"output\": \"Adding all the odd numbers (17, 19) gives 36. The answer is True.\"},\n",
        "    {\"input\": \"The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\", \"output\": \"Adding all the odd numbers (11, 13) gives 24. The answer is True.\"},\n",
        "    {\"input\": \"The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\", \"output\": \"Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\"},\n",
        "    ]\n",
        "\n",
        "# This is a prompt template used to format each individual example.\n",
        "example_prompt = ChatPromptTemplate.from_messages([(\"human\", \"{input}\"), (\"ai\", \"{output}\"),])\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "question = \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 2. True or false.\" #Feel free to add \"let's think step by step\" here\n",
        "\n",
        "final_prompt = ChatPromptTemplate.from_messages([\n",
        "    # (\"system\", \"You are a wondrous wizard of math.\"), #Optional\n",
        "     few_shot_prompt, (\"human\", \"{question}\"),\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "Z4k9sovfHAhA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = final_prompt | ChatOpenAI(temperature=0.0, openai_api_key=openai_api_key)\n",
        "print(chain.invoke({\"question\": question}).content)"
      ],
      "metadata": {
        "id": "RIxd_miOeZLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0d881e-88ab-4449-9d26-091ec93ecfc6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding all the odd numbers (15, 5, 13, 7) gives 40. The answer is True.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz-Z8nIq4iV-"
      },
      "source": [
        "#Self-Consistency"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try with a more complicated example"
      ],
      "metadata": {
        "id": "c-r7j4xK3OeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate.from_template(\n",
        "    \"{information} {question}\"\n",
        ")\n",
        "chain = prompt | ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "responses = []\n",
        "\n",
        "information = \"When I was 6 my sister was half of my age and my mom was 5 times my age. My dad was born two years later than my mom, and my uncle and my dad are twins.\"\n",
        "question = \"Now I'm 50, how old is my uncle? Let's think step by step\"\n",
        "\n",
        "for _ in range(7):\n",
        "  response = chain.invoke({\"information\": information, \"question\": question}).content\n",
        "  print(f\"{response}\\n------------------------------------------------------------\")\n",
        "  responses.append(response)\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"These are previous responses for the given question: {information} {question}: {responses}. Based on these responses, {question}. Pick the most frequent answer.\"\n",
        ")\n",
        "\n",
        "chain = prompt | ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "print(chain.invoke({\"information\": information, \"question\": question, \"responses\": responses}).content)"
      ],
      "metadata": {
        "id": "lw7TcVAehQfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc281718-3a47-4ea6-c5c6-1a08072ec683"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When you were 6, your sister was half of your age, so she was 3 years old.\n",
            "At that time, your mom was 5 times your age, so she was 30 years old.\n",
            "Your dad was born two years later than your mom, so he was 28 years old.\n",
            "Since your dad and your uncle are twins, your uncle was also 28 years old at that time.\n",
            "Now, you are 50 years old, which means 44 years have passed since you were 6.\n",
            "Therefore, your uncle is now 28 + 44 = 72 years old.\n",
            "------------------------------------------------------------\n",
            "When you were 6, your sister was half of your age, so she was 3 years old.\n",
            "At that time, your mom was 5 times your age, so she was 30 years old.\n",
            "Your dad was born two years later than your mom, so he was 28 years old.\n",
            "Since your dad and your uncle are twins, your uncle was also 28 years old at that time.\n",
            "Now, you are 50 years old, which means 44 years have passed since you were 6.\n",
            "Therefore, your uncle is now 28 + 44 = 72 years old.\n",
            "------------------------------------------------------------\n",
            "When you were 6, your sister was half of your age, so she was 3 years old.\n",
            "At that time, your mom was 5 times your age, so she was 30 years old.\n",
            "Your dad was born two years later than your mom, so he was 28 years old.\n",
            "Since your dad and your uncle are twins, your uncle was also 28 years old at that time.\n",
            "Now, you are 50 years old, which means 44 years have passed since you were 6.\n",
            "Therefore, your uncle is now 28 + 44 = 72 years old.\n",
            "------------------------------------------------------------\n",
            "When you were 6, your sister was half of your age, so she was 3 years old.\n",
            "At that time, your mom was 5 times your age, so she was 5 * 6 = 30 years old.\n",
            "Your dad was born two years later than your mom, so he was 30 + 2 = 32 years old.\n",
            "Since your dad and your uncle are twins, your uncle is also 32 years old.\n",
            "Now, you are 50 years old, which means 50 - 6 = 44 years have passed since you were 6.\n",
            "Therefore, your uncle is now 32 + 44 = 76 years old.\n",
            "------------------------------------------------------------\n",
            "When you were 6, your sister was half of your age, so she was 3 years old.\n",
            "At that time, your mom was 5 times your age, so she was 30 years old.\n",
            "Your dad was born two years later than your mom, so he was 28 years old.\n",
            "Since your dad and your uncle are twins, your uncle was also 28 years old at that time.\n",
            "\n",
            "Now, you are 50 years old, which means 44 years have passed since you were 6.\n",
            "Therefore, your uncle is now 28 + 44 = 72 years old.\n",
            "------------------------------------------------------------\n",
            "When you were 6, your sister was half of your age, so she was 3 years old.\n",
            "At that time, your mom was 5 times your age, so she was 5 * 6 = 30 years old.\n",
            "Your dad was born two years later than your mom, so he was 30 + 2 = 32 years old.\n",
            "Since your dad and your uncle are twins, your uncle is also 32 years old.\n",
            "Now, you are 50 years old, which means 50 - 6 = 44 years have passed since you were 6.\n",
            "Therefore, your uncle is now 32 + 44 = 76 years old.\n",
            "------------------------------------------------------------\n",
            "When you were 6, your sister was half of your age, so she was 3 years old.\n",
            "At that time, your mom was 5 times your age, so she was 30 years old.\n",
            "Your dad was born two years later than your mom, so he was 28 years old.\n",
            "Since your dad and your uncle are twins, your uncle was also 28 years old at that time.\n",
            "\n",
            "Now, you are 50 years old, which means 44 years have passed since you were 6.\n",
            "Therefore, your uncle is now 28 + 44 = 72 years old.\n",
            "------------------------------------------------------------\n",
            "The most frequent answer is: \"When you were 6, your sister was half of your age, so she was 3 years old. At that time, your mom was 5 times your age, so she was 30 years old. Your dad was born two years later than your mom, so he was 28 years old. Since your dad and your uncle are twins, your uncle was also 28 years old at that time. Now, you are 50 years old, which means 44 years have passed since you were 6. Therefore, your uncle is now 28 + 44 = 72 years old.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyPVExQa4m0K"
      },
      "source": [
        "#Generate Knowledge Prompting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate.from_template(\"{request}\")\n",
        "\n",
        "request=\"Generate 4 facts about the Kermode bear, then use these facts to write a short blog post using the information? Please write the post in pure text form.\"\n",
        "chain = prompt | ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "print(chain.invoke({\"request\": request}).content)"
      ],
      "metadata": {
        "id": "Fls7Xlngs4-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18ce3ba-0648-4c70-a2e3-d4a789093a1d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fact 1: The Kermode bear, also known as the \"spirit bear\" or \"ghost bear,\" is a rare subspecies of the American black bear.\n",
            "\n",
            "Fact 2: The Kermode bear is primarily found in the Great Bear Rainforest of British Columbia, Canada, which is one of the largest temperate rainforests in the world.\n",
            "\n",
            "Fact 3: What makes the Kermode bear unique is its white or cream-colored fur, which is caused by a recessive gene. Approximately 1 in 10 Kermode bears have this distinctive coat color.\n",
            "\n",
            "Fact 4: The Kermode bear holds great cultural significance for the indigenous people of the region, who consider it a sacred and spiritual animal.\n",
            "\n",
            "---\n",
            "\n",
            "Blog Post: Discover the Enchanting World of the Kermode Bear\n",
            "\n",
            "Welcome, nature enthusiasts! Today, we embark on a journey to explore the captivating realm of the Kermode bear, a remarkable creature that roams the lush forests of British Columbia, Canada. Known for its ethereal beauty and cultural significance, the Kermode bear, also referred to as the \"spirit bear\" or \"ghost bear,\" is a true marvel of nature.\n",
            "\n",
            "Fact 1: The Kermode bear is a rare subspecies of the American black bear. While its black-furred relatives are more commonly seen, the Kermode bear stands out with its stunning white or cream-colored coat. This unique coloration is caused by a recessive gene, making these bears a sight to behold.\n",
            "\n",
            "Fact 2: The Great Bear Rainforest in British Columbia serves as the primary habitat for the Kermode bear. This vast expanse of temperate rainforest, spanning over 6.4 million hectares, provides the perfect environment for these majestic creatures to thrive. With its towering trees, pristine rivers, and abundant wildlife, this region is a true haven for nature lovers.\n",
            "\n",
            "Fact 3: Approximately 1 in 10 Kermode bears possess the coveted white fur, making them a rare sight even within their own subspecies. As you venture into the Great Bear Rainforest, keep your eyes peeled for these enchanting creatures. Witnessing a Kermode bear gracefully moving through the forest is an experience that will leave you in awe.\n",
            "\n",
            "Fact 4: The Kermode bear holds immense cultural significance for the indigenous people of the region. To them, this bear is a sacred and spiritual animal, often considered a symbol of harmony and balance in nature. The legends and stories surrounding the Kermode bear have been passed down through generations, further enhancing the mystique surrounding these magnificent beings.\n",
            "\n",
            "As we conclude our exploration of the Kermode bear, we invite you to immerse yourself in the wonders of the Great Bear Rainforest. Whether you are an avid wildlife enthusiast, a nature photographer, or simply someone seeking solace in the beauty of the natural world, encountering a Kermode bear is an experience that will stay with you forever.\n",
            "\n",
            "Remember, the Kermode bear is a symbol of the delicate balance that exists within our ecosystems. Let us cherish and protect these incredible creatures, ensuring that future generations can continue to marvel at their splendor. Together, let us celebrate the spirit bear and the magic it brings to our world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op9aNUqJ4qHJ"
      },
      "source": [
        "#Tree of Thoughts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import Markdown\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "brainstrom_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Step1 :\n",
        "    I have a thinking about {topic}. Could you brainstorm three distinct solutions?\"\"\"\n",
        ")\n",
        "\n",
        "evaluate_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Step 2:\n",
        "    For each of the three proposed solutions, evaluate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assign a probability of success and a confidence level to each option based on these factors\n",
        "\n",
        "    {solutions}\"\"\"\n",
        ")\n",
        "\n",
        "think_through_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Step 3:\n",
        "    For each solution, deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
        "\n",
        "    {evaluations}\"\"\"\n",
        ")\n",
        "\n",
        "rank_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Step 4:\n",
        "    Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution\n",
        "\n",
        "    {deeper_thoughts}\"\"\"\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "chat_parser = chat | StrOutputParser()\n",
        "\n",
        "# Chain #1: User input passed through as topic, the prompt runs through the llm, and the output stored as solutions.\n",
        "brainstorm_solutions =  {\"topic\": RunnablePassthrough()} | brainstrom_prompt | {\"solutions\": chat_parser}\n",
        "\n",
        "# Chain #2: The output \"solutions\" from brainstrom_solutions fits in the \"solutions\" variable in the prompt, goes through llm and the output stored as \"review\".\n",
        "evaluate_solutions = {\"solutions\": brainstorm_solutions} | evaluate_prompt | {\"review\": chat_parser}\n",
        "\n",
        "# Chain #3\n",
        "think_through = {\"evaluations\": evaluate_solutions} | think_through_prompt | {\"deeper_thoughts\": chat_parser}\n",
        "\n",
        "# Chain #4\n",
        "rank = {\"deeper_thoughts\": think_through} | rank_prompt | chat\n",
        "\n",
        "# Chain them together!\n",
        "chain = ( brainstorm_solutions | evaluate_solutions | think_through | rank)\n",
        "\n",
        "topic = \"how to go to Mars\"\n",
        "response = chain.invoke({\"topic\": topic})\n",
        "\n",
        "\n",
        "Markdown(response.content)\n"
      ],
      "metadata": {
        "id": "hJga3etkTaVL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "cfa0e652-0e57-45aa-a2fd-d6ec83d6ddef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Ranking of Solutions in Order of Promise:\n\n1. Solution 1: Genetic Modification\n2. Solution 2: Underground Habitats\n3. Solution 3: 3D Printing\n\nJustification for Ranking:\n\n1. Solution 1: Genetic Modification\nThis solution ranks first in promise due to its potential to significantly enhance the resilience and survival rates of humans on Mars. The ability to modify human DNA to withstand radiation, extreme temperatures, and low gravity could greatly increase the chances of long-term colonization. However, ethical concerns and public acceptance are significant obstacles that need to be addressed through transparent communication and strict regulations.\n\n2. Solution 2: Underground Habitats\nThe construction of underground habitats ranks second in promise as it offers a controlled and protected environment for human habitation on Mars. These habitats can provide protection from radiation, extreme temperatures, and micrometeorites. However, ensuring structural integrity and addressing potential psychological impacts are challenges that need to be overcome through advanced construction techniques, materials, and psychological support.\n\n3. Solution 3: 3D Printing\n3D printing ranks third in promise as it has the potential to rapidly and efficiently construct structures on Mars using Martian soil as a construction material. This solution reduces reliance on Earth for materials and enables the construction of necessary structures. However, the challenges of adapting 3D printing technology to the Martian environment and ensuring the quality and durability of printed structures need to be addressed through extensive research and development.\n\nFinal Thoughts and Considerations:\n\n- Genetic Modification: The potential benefits of genetic modification are significant, but ethical concerns and public acceptance are major obstacles. Transparent communication and strict regulations are necessary to address these concerns and ensure the ethical use of genetic engineering.\n\n- Underground Habitats: While underground habitats offer protection from external hazards, ensuring structural integrity and addressing potential psychological impacts are crucial. Collaboration with construction companies, engineering firms, and mental health experts is necessary to overcome these challenges.\n\n- 3D Printing: The use of 3D printing technology can revolutionize construction on Mars, but modifications to existing technology and ensuring the quality of printed structures are important considerations. Continuous research and development are needed to improve scalability and complexity, and alternative construction methods should be explored as backups.\n\nOverall, each solution has its own promise and challenges. A combination of these solutions may provide the best approach for long-term colonization on Mars, taking into account the unique conditions and requirements of the planet."
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
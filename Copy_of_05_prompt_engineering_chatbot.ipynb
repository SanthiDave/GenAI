{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot Development\n",
        "\n",
        "In this tutorial, we will go over chatbot development. Chatbot requires multi-turn conversations i.e., model has to remember previous conversations. We will show how to setup system, user, and assistant prompts to make the model capable of seamless conversation."
      ],
      "metadata": {
        "id": "X0nBU8JvDY4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai gradio"
      ],
      "metadata": {
        "id": "yYabiYrXfW32"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from rich import print\n",
        "import json"
      ],
      "metadata": {
        "id": "9hmuzW3CfbKH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('openai_api_key')"
      ],
      "metadata": {
        "id": "Z_lTsQpPfcw_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function for messages\n",
        "\n",
        "In this function, we will use a list of messages instead of a single prompt like the previous tutorials."
      ],
      "metadata": {
        "id": "tMzW2Z_3DgMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(messages, temperature: float =0.0, max_tokens:int = 50):\n",
        "\n",
        "  try:\n",
        "    # Create an OpenAI client using the API key\n",
        "    client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "    # Generate a response using the OpenAI chat completions API, specifying model, messages, temperature, and max_tokens\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "  except Exception as error:\n",
        "    # Handle any exceptions that may occur during API usage and print the error message\n",
        "    print(error)\n",
        "\n",
        "  # Return the content of the generated response as a string\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "6FBTR4GpfmDY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMs are stateless\n",
        "\n",
        "Language models don't have any memory or remember the previous state by design. They are stateless. Let's demonstrate that with an example."
      ],
      "metadata": {
        "id": "KuPVkGhtkA76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "prompt = \"Hey there! My name is Santi!\"\n",
        "messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "FE5a8uWLftlp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "00a32a41-e32d-4456-d375-f4d40dbb12d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Hi Santi! Nice to meet you. How can I assist you today?\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hi Santi! Nice to meet you. How can I assist you today?\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "prompt = \"Hey there! What is my name?\"\n",
        "messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "TIc2XY6GgR1E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2aa029a5-1161-4aa8-d225-250f644af3e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "I'm sorry, I don't know your name. Can you please tell me?\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I'm sorry, I don't know your name. Can you please tell me?\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keep track of history\n",
        "\n",
        "In order to keep track of history, we need to append the previous user inputs and assistant responses and pass the entire thread to the model. This can be done by incrementally adding `user` prompt and `assistant` response to messages list and pass the messages list to the model."
      ],
      "metadata": {
        "id": "JjanCwIQkR8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "prompt = \"Hey there! My name is Santi!\"\n",
        "messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "response = generate_response(messages)\n",
        "messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "prompt = \"Hey there! What is my name?\"\n",
        "messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "MivD2ZK3gYSU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "03c8c28c-69b8-4c71-98d6-4fd7c5460b71"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Your name is Santi.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your name is Santi.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(messages)"
      ],
      "metadata": {
        "id": "0J2q6L0zglk9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "e83e62b4-b6ba-48ff-ed95-ff467fb73ce7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'Hey there! My name is Santi!'\u001b[0m\u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'Hi Santi! Nice to meet you. How can I assist you today?'\u001b[0m\u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'Hey there! What is my name?'\u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Hey there! My name is Santi!'</span><span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Hi Santi! Nice to meet you. How can I assist you today?'</span><span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Hey there! What is my name?'</span><span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Chatbot\n",
        "\n",
        "Let's build a basic chatbot to demonstrate this back and forth.\n",
        "\n",
        "We will use [Gradio](https://www.gradio.app/docs/chatinterface) to build a basic chatbot.\n",
        "\n",
        "Function `chatbot_response`:\n",
        "\n",
        "a. takes a user message and conversation history as arguments.  \n",
        "b. organizes the messages into a conversation format, generates a response and returns it.\n",
        "\n",
        "A Gradio chat interface named demo is created, allowing users to interact with the chatbot, and it's launched for use."
      ],
      "metadata": {
        "id": "O44A42lqDmKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chatbot_response(message, history):\n",
        "\n",
        "  # Create an empty list named messages to store the conversation history.\n",
        "  messages = []\n",
        "\n",
        "  # Iterate through each entry in the history list.\n",
        "  for entry in history:\n",
        "\n",
        "    # Append a dictionary to the messages list representing a user message with the content from entry[0].\n",
        "    messages.append({\"role\" : \"user\", \"content\": entry[0]})\n",
        "\n",
        "    # Append another dictionary to the messages list representing an assistant message with the content from entry[1].\n",
        "    messages.append({\"role\" : \"assistant\", \"content\": entry[1]})\n",
        "\n",
        "  # Append a new dictionary to the messages list representing the current user message with the content from the 'message' argument.\n",
        "  messages.append({\"role\" : \"user\", \"content\" : message})\n",
        "\n",
        "  # Call a function named generate_response with the 'messages' list as an argument to generate a response.\n",
        "  response = generate_response(messages)\n",
        "\n",
        "  # Return the generated response.\n",
        "  return response\n",
        "\n",
        "demo = gr.ChatInterface(chatbot_response)\n",
        "\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "op5JxjjZhKKh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "0da70f4a-8f31-4413-8299-7c616c5573c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://fad7543dcef1ec7bd0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fad7543dcef1ec7bd0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thai Restaurant Order Chatbot\n",
        "\n",
        "Let's do something a bit more complex. Let's make a chatbot, named ThaiBot that can greet customers, collect detailed pizza orders, clarify preferences, handle additional items, and manage payments. For delivery orders, it will request the address. The bot will maintain a friendly, conversational style throughout interactions.\n",
        "\n",
        "In order to build this bot, we will make a system prompt that has all the necessary information to perform its operations."
      ],
      "metadata": {
        "id": "iHkLs4KlDt3J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QVzGM8L-5pNY"
      },
      "outputs": [],
      "source": [
        "system_prompt = {'role':'system', 'content':\"\"\"\n",
        "You are ThaiBot, an automated service for a Thai restaurant. \\\n",
        "You start by warmly greeting the customer, taking their order, and inquiring whether it's for dine-in or takeout. \\\n",
        "You patiently collect the entire order, summarize it, and double-check if there are any additional items the customer would like. If it's for delivery, kindly request the delivery address. Finally, you process the payment. Please ensure to clarify all menu options, extras, and portion sizes for a seamless ordering experience. \\\n",
        "Your responses should be brief, friendly, and conversational.\n",
        "\n",
        "The menu features a variety of Thai dishes, including:\n",
        "\n",
        "Pad Thai: 12.95, 10.00, 7.00\n",
        "Green Curry: 10.95, 9.25, 6.50\n",
        "Tom Yum Soup: 11.95, 9.75, 6.75\n",
        "Spring Rolls: 4.50, 3.50\n",
        "Thai Salad: 7.25\n",
        "For customizing your dishes, we offer toppings such as:\n",
        "\n",
        "Extra Tofu: 2.00\n",
        "Fresh Basil: 1.50\n",
        "Shrimp: 3.00\n",
        "Chicken: 3.50\n",
        "Spicy Sauce: 1.50\n",
        "Chili Peppers: 1.00\n",
        "We also have a selection of beverages:\n",
        "\n",
        "Thai Iced Tea: 3.00\n",
        "Coconut Water: 3.00\n",
        "Bottled Water: 5.00\n",
        "\"\"\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function `chatbot_response`:\n",
        "\n",
        "a. takes a user message and conversation history as arguments.  \n",
        "b. organizes the messages into a conversation format, generates a response and returns it.\n",
        "\n",
        "A Gradio chat interface named demo is created, allowing users to interact with the chatbot, and it's launched for use."
      ],
      "metadata": {
        "id": "Hkj6ZhEMr0O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def order_chatbot_response(message, history):\n",
        "\n",
        "  # Create an empty list named messages to store the conversation history.\n",
        "  messages = []\n",
        "\n",
        "  # Append the system_prompt (which should be predefined elsewhere) to the messages list.\n",
        "  messages.append(system_prompt)\n",
        "\n",
        "  # Iterate through each entry in the history list.\n",
        "  for entry in history:\n",
        "    # Append a dictionary to the messages list representing a user message with the content from entry[0].\n",
        "    messages.append({\"role\" : \"user\", \"content\": entry[0]})\n",
        "    # Append another dictionary to the messages list representing an assistant message with the content from entry[1].\n",
        "    messages.append({\"role\" : \"assistant\", \"content\": entry[1]})\n",
        "\n",
        "  # Append a new dictionary to the messages list representing the current user message with the content from the 'message' argument.\n",
        "  messages.append({\"role\" : \"user\", \"content\" : message})\n",
        "\n",
        "  # Call a function named generate_response with the 'messages' list as an argument to generate a response.\n",
        "  response = generate_response(messages)\n",
        "\n",
        "  # Return the generated response.\n",
        "  return response\n",
        "\n",
        "order_demo = gr.ChatInterface(order_chatbot_response)\n",
        "\n",
        "\n",
        "order_demo.launch()"
      ],
      "metadata": {
        "id": "WrGBmqAalo0u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "4c90607d-1245-49ec-c99f-d0b4905e32a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://89551dcf33e45a5207.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://89551dcf33e45a5207.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}